# Retrospective — Epic 2: Fair Scheduling & Message Reliability

**Date:** 2026-02-12
**Facilitator:** Bob (Scrum Master)
**Project Lead:** Lucas

## Epic Summary

| Metric | Value |
|--------|-------|
| Stories | 4/4 completed (100%) |
| Scope | DRR scheduler, weighted fairness verification (proptest), nack/message retry, visibility timeout/lease expiry |
| Test Suite | 92 tests passing, 1 ignored (up from 59 in Epic 1) |
| Architecture Deviations | 0 |
| Technical Debt Items | 2 (O(n²) message scan in delivery, stale sprint-status tracking) |
| Benchmark Results | DRR overhead ~114ns/iter, constant across 1/10/100 keys |
| Fairness Accuracy | Proptest-verified within 5% for arbitrary key/weight combinations (2-20 keys, weights 1-10) |

## Team Participants

- Bob (Scrum Master) — Facilitator
- Alice (Product Owner) — Business perspective
- Charlie (Senior Dev) — Technical lead
- Dana (QA Engineer) — Quality perspective
- Elena (Junior Dev) — Fresh perspective
- Lucas (Project Lead) — Decision maker

## Successes

1. **DRR scheduling validates architecture** — The single-threaded scheduler with DRR delivers ~114ns per-iteration overhead, constant regardless of key count (1/10/100). This proves the Redis-inspired concurrency model works for the core differentiator. We can now say with benchmark data that fair scheduling costs virtually nothing.

2. **Mathematical proof of fairness invariant** — Proptest in Story 2.2 generates arbitrary key counts (2-20) and weights (1-10) under sustained load, verifying each key receives within 5% of its fair share. This is not a spot-check — it's a property-based proof of FR12 that found edge cases targeted tests wouldn't have covered (deficit integer division effects, weight boundary conditions).

3. **Pattern reuse accelerated velocity** — Each story built cleanly on the previous. Story 2.3 (nack) followed the ack pattern from Epic 1. Story 2.4 (lease expiry) followed the nack pattern from 2.3. By Story 2.4, implementation was clean on first attempt with zero debug issues. The codebase is getting easier to work with, not harder.

4. **Zero architecture deviations** — Unlike Epic 1 (which deviated on consumer channels), Epic 2 implemented exactly as designed. Architecture doc was updated before epic started per Epic 1 retro action item. No new storage trait methods were needed — the key encoding from Epic 1 already supported per-fairness-key prefix scans.

5. **Vertical story slicing worked** — Every story delivers observable behavior change: 2.1 (fair delivery), 2.2 (weighted accuracy), 2.3 (retry on failure), 2.4 (automatic lease recovery). No infrastructure-only stories. Direct improvement from Epic 1 retro.

6. **Comprehensive test coverage** — 33 new tests added across the epic (92 total, up from 59). Includes unit tests, integration tests, property-based tests, and benchmarks. All acceptance criteria have corresponding test coverage.

## Challenges

1. **Test design complexity with concurrent scheduler** — 3 out of 4 stories (2.1, 2.2, 2.3) required debugging test timing or ordering issues. The scheduler's concurrent behavior (delivery triggers, channel capacity, consumer registration timing) makes tests hard to reason about. Story 2.2's fairness test deadlocked due to channel capacity (256 << 10,500 commands). Story 2.3 tests had unexpected redelivery due to inline `drr_deliver_queue()` calls. This is not a test design problem — it's a test infrastructure problem. The test helpers from Epic 1 are insufficient for the scheduler's growing complexity.

2. **O(n²) message scan in delivery growing from footnote to constraint** — The `message_prefix_with_key` scan + per-message lease check in `drr_deliver_queue()` is quadratic. In Epic 1 it was a footnote. Now the 10k fairness test takes ~184 seconds in debug and is `#[ignore]` — meaning our core differentiator's full-path verification doesn't run in CI. In Epic 3, Lua `on_enqueue` runs per message during delivery. If Lua adds latency on top of quadratic scanning, it compounds. Escalated from Medium to High priority during this retro.

3. **Sprint-status tracking still inconsistent** — Stories 2-1 and 2-2 show `review` in sprint-status.yaml despite being done per git log. Epic-2 shows `in-progress` instead of `done`. The execute-epic workflow was built (Epic 1 action item) but doesn't reliably update story status to `done`. The automation exists but isn't reliable — likely a workflow edge case.

4. **DRR delivery trigger ordering was a recurring theme** — Stories 2.1, 2.3, and 2.4 all needed careful consideration of when to call `drr_deliver_queue()`. Delivery is triggered in multiple places — inline after commands AND in the main loop. Correct behavior, but non-obvious. This will interact with Lua in Epic 3 (Lua runs inside handle_enqueue/handle_nack, which already trigger delivery), though the call sites don't change — only the timing complexity increases.

## Key Insights

1. **The hardest part of DRR was not the algorithm — it was the integration.** The DRR data structure itself was straightforward (~150 lines). The complexity was wiring it into the scheduler's event loop: when to deliver, when to start new rounds, when to add/remove keys (~500 lines of integration). This pattern will repeat in Epic 3 — the Lua sandbox API will be simpler than integrating it into the enqueue/nack paths.

2. **Property-based testing is essential for core invariants — invest upfront.** The proptest in Story 2.2 found edge cases that targeted tests wouldn't have covered. For Fila's core differentiator, mathematical verification is the right level of confidence. Team agreement: apply this approach to Lua safety testing in Epic 3 (write property tests first, then implement).

3. **Pattern establishment in early stories pays compound returns.** Story 2.1 was the most expensive story (new algorithm + integration). Stories 2.3 and 2.4 were fast because they followed established patterns. Story 2.4 had zero debug issues. Investing in clean patterns in Story 3.1 will pay off for 3.2-3.4.

4. **Test infrastructure must evolve with the system — not ad-hoc per story.** Channel capacity, scheduler threading, consumer timing — each story adapted testing patterns independently rather than investing in shared helpers. Epic 3 needs proper concurrent test infrastructure from the start.

## Previous Retrospective Follow-Through

### Epic 1 Action Items Assessment

| # | Action Item | Status | Evidence |
|---|-------------|--------|----------|
| 1 | Vertical story slicing for Epic 2 | ✅ Completed | All 4 stories deliver observable behavior changes |
| 2 | Enforce test coverage in code review | ✅ Completed | All ACs have corresponding tests; 33 new tests |
| 3 | Story file completion as workflow step | ⏳ Partial | Stories 2-1/2-2 still show `review`; sprint-status stale |
| 4 | Extract shared `fila_error_to_status()` | ✅ Completed | Refactored to `IntoStatus` trait in `fila-server/src/error.rs` |
| 5 | Update architecture doc | ✅ Completed | Commit `e81ff46` before Epic 2 |
| 6 | Add basic README | ✅ Completed | Commit `ee5b63b` before Epic 2 |
| 7 | Build epic execution workflow | ✅ Completed | Commit `bb4dfef` |
| 8 | Proptest spike (Knowledge Dev) | ✅ Completed | Story 2.2 Task 1 |

**Summary:** 7 of 8 action items completed (88%), 1 partially addressed.

**Wins:** Vertical slicing and test enforcement clearly improved Epic 2 quality. The `IntoStatus` refactor happened organically during Story 2.3. Architecture doc update prevented drift — zero deviations.

**Missed:** Sprint-status tracking remains inconsistent despite workflow improvements. The execute-epic workflow was built but doesn't reliably update story status to `done`. Likely a workflow edge case rather than a process gap.

## Next Epic Preview

**Epic 3: Lua Rules Engine & Dead-Letter Handling** — 4 stories

| Story | Description | Key Dependency on Epic 2 |
|-------|-------------|--------------------------|
| 3.1 | Lua Sandbox & on_enqueue Hook | Replaces default fairness key with Lua-computed assignment (builds on 2.1 DRR) |
| 3.2 | Lua Safety: Timeouts, Memory Limits & Circuit Breaker | Sandbox safety for scripts running on hot path |
| 3.3 | on_failure Hook & Retry Decisions | Builds directly on nack flow from Story 2.3 |
| 3.4 | Dead-Letter Queue | Uses message lifecycle from Stories 2.3/2.4; CreateQueue needs atomic dual-queue creation |

**New dependency:** `mlua` crate (Lua 5.4 Rust bindings)

**FRs covered:** FR6, FR18-25, FR32

**Technical complexity:** High — embedding a scripting engine on the scheduler hot path with safety guarantees (timeouts, memory limits, circuit breaker). This is the most technically novel epic after DRR.

**Key concerns surfaced in retro:**
- mlua is an unknown — need spike to verify: bytecode pre-compilation, instruction count timeouts, memory limits via allocator hooks, sandbox function stripping, Rust↔Lua bridge for fila.get()
- Lua runs synchronously inside the scheduler thread. A slow/stuck script blocks everything. The timeout mechanism (instruction count hook) is safety-critical.
- O(n²) delivery scan will compound with per-message Lua execution overhead
- DLQ creation (Story 3.4) requires CreateQueue to atomically create two queues — WriteBatch change in queue creation path

**Dependencies on Epic 2 are solid:** Nack flow (2.3) and lease expiry (2.4) both follow clean patterns with full test coverage. No concerns about building on_failure hooks on top of them.

## Action Items

### Process Improvements

| # | Action | Owner | When | Success Criteria |
|---|--------|-------|------|-----------------|
| 1 | Fix sprint-status update reliability in execute-epic workflow | Bob (SM) | Before Epic 3 story creation | All stories show `done` in sprint-status when complete. Epic status transitions to `done`. Verified by spot-checking after each story. |
| 2 | Invest in test infrastructure for concurrent scheduler testing | Charlie (Dev) | During Epic 3 Story 3.1 | Shared test helpers handle channel sizing, consumer lifecycle, scheduler timing, and Lua script injection. No more ad-hoc test workarounds. |

### Technical Debt

| # | Item | Owner | Priority | Notes |
|---|------|-------|----------|-------|
| 1 | O(n²) message scan in delivery (`drr_deliver_queue`) | Charlie (Dev) | **High** | Caused 184s test runtime in Story 2.2 (core invariant test not in CI). Will compound with Lua overhead in Epic 3. Plan: add per-fairness-key in-memory pending message index during Story 3.1 delivery refactor for Lua integration. |
| 2 | Sprint-status.yaml stale entries fixed | Bob (SM) | ✅ Done | Fixed during this retrospective: stories 2-1/2-2 → done, epic-2 → done, retro → done. |

### Documentation

| # | Item | Owner | Deadline |
|---|------|-------|----------|
| 1 | Document DRR algorithm and fairness guarantees (developer-facing) | Charlie (Dev) | Nice-to-have for Epic 3 |

### Team Agreements

- Test every AC with a corresponding test (continued from Epic 1)
- For Epic 3: write safety property tests (timeout, memory limit, circuit breaker) before implementing the safety mechanisms
- Sprint-status must be updated as the final step of every story completion
- mlua spike completed and reviewed before any Epic 3 story begins

## Critical Path — Before Epic 3

| # | Item | Owner | Blocks |
|---|------|-------|--------|
| 1 | **mlua evaluation spike** | Charlie (Dev) | All of Epic 3 |

Spike scope: build minimal example verifying bytecode pre-compilation, instruction count timeout, memory limit via allocator hook, sandbox function stripping (no IO/OS/filesystem), Rust→Lua bridge (fila.get() pattern).

## Epic 3 Preparation Tasks

**Technical Setup:**
- [ ] mlua evaluation spike (CRITICAL — blocks Epic 3) — Owner: Charlie

**Parallel Work (during early stories):**
- [ ] Design per-fairness-key pending message index to replace O(n²) scan — Owner: Charlie. Implement during Story 3.1.
- [ ] Research Lua testing patterns (circuit breaker timing, memory limits, sandbox safety) — Owner: Dana

**Nice-to-Have:**
- [ ] DLQ atomicity design (CreateQueue creating two queues atomically) — can happen during Story 3.4 planning

## Significant Discovery Assessment

No significant discoveries from Epic 2 that fundamentally change the plan for Epic 3. The architecture's DRR implementation matches the design exactly, and all functional requirements (FR4, FR5, FR8-12) were delivered as specified.

**One observation to monitor:** The O(n²) delivery scan will interact with Lua `on_enqueue` execution in Epic 3. If Lua adds latency per-message, the scan cost compounds. This is why the scan optimization was escalated to High priority and planned for Story 3.1.

**Epic update required:** NO — Epic 3 plan is sound.

## Readiness Assessment

| Area | Status | Details |
|------|--------|---------|
| Testing & Quality | ✅ Ready | 92 tests passing. Core invariant protected by proptest (runs in CI). 10k integration test ignored (known, planned fix via scan optimization). |
| Codebase Health | ✅ Stable | Patterns maturing. DRR integration clean. Story 2.4 had zero debug issues. Codebase getting easier to work with. |
| Architecture Doc | ✅ Current | Updated before Epic 2. No deviations requiring further updates. |
| Sprint-Status | ✅ Fixed | Corrected during this retrospective. |
| Epic 2 Foundations | ✅ Solid | Nack flow and lease expiry both clean with full test coverage. Safe to build Lua hooks on top. |
| Unresolved Blockers | ✅ None | O(n²) scan has a plan (Story 3.1). mlua is unknown but spike is on critical path. |

**Overall:** Epic 2 is fully complete. One prep task (mlua spike) before Epic 3 can begin. All Epic 2 foundations that Epic 3 depends on are solid and well-tested.

## Closure

**Key Takeaways:**

1. DRR scheduling validates the single-threaded architecture at ~114ns/iter — Fila's core differentiator is real and performant
2. Property-based testing is essential for core invariants — apply to Lua safety in Epic 3
3. Pattern establishment in early stories pays compound returns — invest in clean patterns in Story 3.1
4. Test infrastructure must evolve with the system — build concurrent test helpers at Epic 3 start

**Commitments:**
- Action Items: 3 (process: 2, technical debt: 1 remaining)
- Preparation Tasks: 3 (1 critical, 2 parallel)
- Critical Path Items: 1 (mlua spike)
- Team Agreements: 4

**Next Steps:**

1. Complete mlua evaluation spike (critical path)
2. Begin Epic 3 story creation after spike review
3. Address O(n²) scan during Story 3.1 delivery refactor
4. Review action items in next standup

**Epic 2 delivered Fila's core differentiator.** DRR fair scheduling with mathematically verified fairness guarantees is now operational. The single-threaded scheduler architecture is validated with benchmark data. Epic 3 will add the user-facing policy layer (Lua) that makes this primitive configurable — the epic that turns a scheduling engine into a product.
